{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy7VuK2OaMNx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDjwLqoAwiX0",
        "outputId": "c942587b-4c6f-4354-f4da-2b62299326d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.19.1-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.1/311.1 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from attrdict) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tensorboardX, smmap, safetensors, attrdict, huggingface-hub, gitdb, tokenizers, GitPython, transformers\n",
            "Successfully installed GitPython-3.1.40 attrdict-2.0.1 gitdb-4.0.11 huggingface-hub-0.17.3 safetensors-0.4.0 smmap-5.0.1 tensorboardX-2.6.2.2 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers GitPython attrdict tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpV3VGbzvJqr"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, BertForTokenClassification, BertModel, RobertaModel, RobertaTokenizer, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import git\n",
        "import zipfile\n",
        "import logging\n",
        "import pdb\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "import random\n",
        "from time import time\n",
        "from gensim import models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5DiPcHjx7qt",
        "outputId": "66b9b58d-c8cb-46a0-f68f-65deec58ddf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Cloning, or repository already cloned\n"
          ]
        }
      ],
      "source": [
        "PROJECT_DIR = 'drive/MyDrive/Shreya Data/Shreya NLP/Project'\n",
        "DATA_DIR = 'data/SVAMP.json'\n",
        "# Cloning reference repository\n",
        "repo_url = 'https://github.com/arkilpatel/SVAMP.git'\n",
        "try:\n",
        "    git.Repo.clone_from(repo_url, os.path.join(PROJECT_DIR,'reference'))\n",
        "except:\n",
        "    print(\"Error Cloning, or repository already cloned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VZqBcJCwuvX"
      },
      "outputs": [],
      "source": [
        "# Load the SVAMP dataset\n",
        "with open(os.path.join(PROJECT_DIR,DATA_DIR),'r') as f:\n",
        "    svamp_data = json.load(f)\n",
        "\n",
        "# Extract texts and labels (in your case, texts are problem statements)\n",
        "texts = [item[\"Body\"] + \" \" + item[\"Question\"] for item in svamp_data]\n",
        "labels = [item[\"Answer\"] for item in svamp_data]  # You may need to adjust based on your data structure\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYS4aTQc77S2"
      },
      "outputs": [],
      "source": [
        "CODE_DIR = os.path.join(PROJECT_DIR,'reference/code/transformer_seq2seq/src')\n",
        "if CODE_DIR not in sys.path:\n",
        "    sys.path.append(CODE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Zds2dgRo-6se",
        "outputId": "02fe9e02-5905-43bd-e8fe-ea7ef7814d06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/MyDrive/Shreya Data/Shreya NLP/Project/reference/code/requirements.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "req_dir = os.path.join(PROJECT_DIR,'reference/code/requirements.txt')\n",
        "req_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59LfWV2O77Q_"
      },
      "outputs": [],
      "source": [
        "# from args import build_parser\n",
        "from utils.helper import *\n",
        "from utils.logger import get_logger, print_log, store_results, store_val_results\n",
        "from dataloader import TextDataset\n",
        "from model import build_model, train_model, run_validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jjxxg-vwILlv"
      },
      "outputs": [],
      "source": [
        "import main as m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd5OHmLdgsaC"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "        'mode': 'train',\n",
        "        'debug': False,\n",
        "        'run_name': 'BERT-h10-d300-ep10',\n",
        "        'dataset': 'cv_svamp_augmented',\n",
        "        'display_freq': 10000,\n",
        "        'outputs': True,\n",
        "        'results': True,\n",
        "        'vocab_size': 30000,\n",
        "        'histogram': False,\n",
        "        'save_writer': False,\n",
        "        'gpu': 0,\n",
        "        'early_stopping': 500,\n",
        "        'seed': 6174,\n",
        "        'logging': 1,\n",
        "        'ckpt': None,\n",
        "        'save_model': False,\n",
        "        'heads': 8, #6\n",
        "        'encoder_layers': 4,\n",
        "        'decoder_layers': 4,\n",
        "        'd_model': 300,\n",
        "        'd_ff': 1200,\n",
        "        'lr': 1e-5,\n",
        "        'dropout': 0.1,\n",
        "        'warmup': 0.1,\n",
        "        'max_grad_norm': 0.25,\n",
        "        'batch_size': 8,\n",
        "        'max_length': 100,\n",
        "        'init_range': 0.08,\n",
        "        'embedding': 'bert',#'word2vec',\n",
        "        'word2vec_bin': '/content/drive/MyDrive/Shreya Data/Shreya NLP/Project/data/GoogleNews-vectors-negative300.bin',#'/datadrive/satwik/global_data/GoogleNews-vectors-negative300.bin',\n",
        "        'emb_name': 'bert-base-uncased',#'roberta-base',\n",
        "        'emb_lr': 3e-5,\n",
        "        'freeze_emb': False,\n",
        "        'epochs': 10,\n",
        "        'opt': 'adamw',\n",
        "        'grade_disp': False,\n",
        "        'type_disp':False,\n",
        "        'challenge_disp': False,\n",
        "        'nums_disp': False,\n",
        "        'more_nums': False,\n",
        "        'mawps_vocab': False,\n",
        "        'show_train_acc': torch.triu_indices,\n",
        "        'full_cv': True,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvkcSMGiEtCh",
        "outputId": "3a940559-8fe3-4c8b-fdf5-42ee58b41a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:18:20,090 | DEBUG | main.py: 153 : main() ::\t Created Relevant Directories\n",
            "DEBUG:BERT-h10-d300-ep10_fold0:Created Relevant Directories\n",
            "2023-11-13 18:18:20,093 | INFO | main.py: 154 : main() ::\t Experiment Name: BERT-h10-d300-ep10\n",
            "INFO:BERT-h10-d300-ep10_fold0:Experiment Name: BERT-h10-d300-ep10\n",
            "2023-11-13 18:18:20,101 | DEBUG | main.py: 69 : load_data() ::\t Loading Training Data...\n",
            "DEBUG:BERT-h10-d300-ep10_fold0:Loading Training Data...\n",
            "2023-11-13 18:18:20,223 | INFO | main.py: 86 : load_data() ::\t Training and Validation Data Loaded:\n",
            "Train Size: 3936\n",
            "Val Size: 208\n",
            "INFO:BERT-h10-d300-ep10_fold0:Training and Validation Data Loaded:\n",
            "Train Size: 3936\n",
            "Val Size: 208\n",
            "2023-11-13 18:18:20,231 | DEBUG | main.py: 160 : main() ::\t Creating Vocab...\n",
            "DEBUG:BERT-h10-d300-ep10_fold0:Creating Vocab...\n",
            "2023-11-13 18:18:22,405 | INFO | main.py: 174 : main() ::\t Vocab Created with number of words : 4068\n",
            "INFO:BERT-h10-d300-ep10_fold0:Vocab Created with number of words : 4068\n",
            "2023-11-13 18:18:22,428 | INFO | main.py: 180 : main() ::\t Vocab saved at drive/MyDrive/Shreya Data/Shreya NLP/Project/models/BERT-h10-d300-ep10_fold0/vocab1.p\n",
            "INFO:BERT-h10-d300-ep10_fold0:Vocab saved at drive/MyDrive/Shreya Data/Shreya NLP/Project/models/BERT-h10-d300-ep10_fold0/vocab1.p\n",
            "2023-11-13 18:18:22,476 | DEBUG | main.py: 153 : main() ::\t Created Relevant Directories\n",
            "DEBUG:BERT-h10-d300-ep10_fold1:Created Relevant Directories\n",
            "2023-11-13 18:18:22,480 | INFO | main.py: 154 : main() ::\t Experiment Name: BERT-h10-d300-ep10\n",
            "INFO:BERT-h10-d300-ep10_fold1:Experiment Name: BERT-h10-d300-ep10\n",
            "2023-11-13 18:18:22,482 | DEBUG | main.py: 69 : load_data() ::\t Loading Training Data...\n",
            "DEBUG:BERT-h10-d300-ep10_fold1:Loading Training Data...\n",
            "2023-11-13 18:18:22,647 | INFO | main.py: 86 : load_data() ::\t Training and Validation Data Loaded:\n",
            "Train Size: 3920\n",
            "Val Size: 224\n",
            "INFO:BERT-h10-d300-ep10_fold1:Training and Validation Data Loaded:\n",
            "Train Size: 3920\n",
            "Val Size: 224\n",
            "2023-11-13 18:18:22,668 | DEBUG | main.py: 160 : main() ::\t Creating Vocab...\n",
            "DEBUG:BERT-h10-d300-ep10_fold1:Creating Vocab...\n",
            "2023-11-13 18:18:25,631 | INFO | main.py: 174 : main() ::\t Vocab Created with number of words : 4072\n",
            "INFO:BERT-h10-d300-ep10_fold1:Vocab Created with number of words : 4072\n",
            "2023-11-13 18:18:25,690 | INFO | main.py: 180 : main() ::\t Vocab saved at drive/MyDrive/Shreya Data/Shreya NLP/Project/models/BERT-h10-d300-ep10_fold1/vocab1.p\n",
            "INFO:BERT-h10-d300-ep10_fold1:Vocab saved at drive/MyDrive/Shreya Data/Shreya NLP/Project/models/BERT-h10-d300-ep10_fold1/vocab1.p\n",
            "2023-11-13 18:18:25,863 | DEBUG | main.py: 153 : main() ::\t Created Relevant Directories\n",
            "DEBUG:BERT-h10-d300-ep10_fold2:Created Relevant Directories\n",
            "2023-11-13 18:18:25,869 | INFO | main.py: 154 : main() ::\t Experiment Name: BERT-h10-d300-ep10\n",
            "INFO:BERT-h10-d300-ep10_fold2:Experiment Name: BERT-h10-d300-ep10\n",
            "2023-11-13 18:18:25,873 | DEBUG | main.py: 69 : load_data() ::\t Loading Training Data...\n",
            "DEBUG:BERT-h10-d300-ep10_fold2:Loading Training Data...\n",
            "2023-11-13 18:18:26,050 | INFO | main.py: 86 : load_data() ::\t Training and Validation Data Loaded:\n",
            "Train Size: 3912\n",
            "Val Size: 232\n",
            "INFO:BERT-h10-d300-ep10_fold2:Training and Validation Data Loaded:\n",
            "Train Size: 3912\n",
            "Val Size: 232\n",
            "2023-11-13 18:18:26,058 | DEBUG | main.py: 160 : main() ::\t Creating Vocab...\n",
            "DEBUG:BERT-h10-d300-ep10_fold2:Creating Vocab...\n",
            "2023-11-13 18:18:29,534 | INFO | main.py: 174 : main() ::\t Vocab Created with number of words : 4074\n",
            "INFO:BERT-h10-d300-ep10_fold2:Vocab Created with number of words : 4074\n",
            "2023-11-13 18:18:29,578 | INFO | main.py: 180 : main() ::\t Vocab saved at drive/MyDrive/Shreya Data/Shreya NLP/Project/models/BERT-h10-d300-ep10_fold2/vocab1.p\n",
            "INFO:BERT-h10-d300-ep10_fold2:Vocab saved at drive/MyDrive/Shreya Data/Shreya NLP/Project/models/BERT-h10-d300-ep10_fold2/vocab1.p\n",
            "2023-11-13 18:18:29,661 | DEBUG | main.py: 153 : main() ::\t Created Relevant Directories\n",
            "DEBUG:BERT-h10-d300-ep10_fold3:Created Relevant Directories\n",
            "2023-11-13 18:18:29,686 | INFO | main.py: 154 : main() ::\t Experiment Name: BERT-h10-d300-ep10\n",
            "INFO:BERT-h10-d300-ep10_fold3:Experiment Name: BERT-h10-d300-ep10\n",
            "2023-11-13 18:18:29,688 | DEBUG | main.py: 69 : load_data() ::\t Loading Training Data...\n",
            "DEBUG:BERT-h10-d300-ep10_fold3:Loading Training Data...\n",
            "2023-11-13 18:18:29,859 | INFO | main.py: 86 : load_data() ::\t Training and Validation Data Loaded:\n",
            "Train Size: 3976\n",
            "Val Size: 168\n",
            "INFO:BERT-h10-d300-ep10_fold3:Training and Validation Data Loaded:\n",
            "Train Size: 3976\n",
            "Val Size: 168\n",
            "2023-11-13 18:18:29,864 | DEBUG | main.py: 160 : main() ::\t Creating Vocab...\n",
            "DEBUG:BERT-h10-d300-ep10_fold3:Creating Vocab...\n",
            "2023-11-13 18:18:34,001 | INFO | main.py: 174 : main() ::\t Vocab Created with number of words : 4077\n",
            "INFO:BERT-h10-d300-ep10_fold3:Vocab Created with number of words : 4077\n",
            "2023-11-13 18:18:34,024 | INFO | main.py: 180 : main() ::\t Vocab saved at drive/MyDrive/Shreya Data/Shreya NLP/Project/models/BERT-h10-d300-ep10_fold3/vocab1.p\n",
            "INFO:BERT-h10-d300-ep10_fold3:Vocab saved at drive/MyDrive/Shreya Data/Shreya NLP/Project/models/BERT-h10-d300-ep10_fold3/vocab1.p\n",
            "2023-11-13 18:18:34,076 | DEBUG | main.py: 153 : main() ::\t Created Relevant Directories\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Created Relevant Directories\n",
            "2023-11-13 18:18:34,080 | INFO | main.py: 154 : main() ::\t Experiment Name: BERT-h10-d300-ep10\n",
            "INFO:BERT-h10-d300-ep10_fold4:Experiment Name: BERT-h10-d300-ep10\n",
            "2023-11-13 18:18:34,083 | DEBUG | main.py: 69 : load_data() ::\t Loading Training Data...\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Loading Training Data...\n",
            "2023-11-13 18:18:34,270 | INFO | main.py: 86 : load_data() ::\t Training and Validation Data Loaded:\n",
            "Train Size: 3960\n",
            "Val Size: 184\n",
            "INFO:BERT-h10-d300-ep10_fold4:Training and Validation Data Loaded:\n",
            "Train Size: 3960\n",
            "Val Size: 184\n",
            "2023-11-13 18:18:34,278 | DEBUG | main.py: 160 : main() ::\t Creating Vocab...\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Creating Vocab...\n",
            "2023-11-13 18:18:36,951 | INFO | main.py: 174 : main() ::\t Vocab Created with number of words : 4077\n",
            "INFO:BERT-h10-d300-ep10_fold4:Vocab Created with number of words : 4077\n",
            "2023-11-13 18:18:36,984 | INFO | main.py: 180 : main() ::\t Vocab saved at drive/MyDrive/Shreya Data/Shreya NLP/Project/models/BERT-h10-d300-ep10_fold4/vocab1.p\n",
            "INFO:BERT-h10-d300-ep10_fold4:Vocab saved at drive/MyDrive/Shreya Data/Shreya NLP/Project/models/BERT-h10-d300-ep10_fold4/vocab1.p\n",
            "2023-11-13 18:18:36,989 | DEBUG | main.py: 69 : load_data() ::\t Loading Training Data...\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Loading Training Data...\n",
            "2023-11-13 18:18:37,103 | INFO | main.py: 86 : load_data() ::\t Training and Validation Data Loaded:\n",
            "Train Size: 3960\n",
            "Val Size: 184\n",
            "INFO:BERT-h10-d300-ep10_fold4:Training and Validation Data Loaded:\n",
            "Train Size: 3960\n",
            "Val Size: 184\n",
            "2023-11-13 18:18:37,112 | INFO | main.py: 184 : main() ::\t Loading Vocab File...\n",
            "INFO:BERT-h10-d300-ep10_fold4:Loading Vocab File...\n",
            "2023-11-13 18:18:37,129 | INFO | main.py: 191 : main() ::\t Vocab Files loaded from drive/MyDrive/Shreya Data/Shreya NLP/Project/models/BERT-h10-d300-ep10_fold4/vocab1.p\n",
            "Number of Words: 4077\n",
            "INFO:BERT-h10-d300-ep10_fold4:Vocab Files loaded from drive/MyDrive/Shreya Data/Shreya NLP/Project/models/BERT-h10-d300-ep10_fold4/vocab1.p\n",
            "Number of Words: 4077\n",
            "2023-11-13 18:18:37,140 | WARNING | helper.py: 65 : get_latest_checkpoint() ::\t No Checkpoints Found\n",
            "WARNING:BERT-h10-d300-ep10_fold4:No Checkpoints Found\n",
            "2023-11-13 18:18:37,146 | DEBUG | model.py: 65 : __init__() ::\t Initialising Embeddings.....\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Initialising Embeddings.....\n",
            "2023-11-13 18:18:40,523 | DEBUG | model.py: 88 : __init__() ::\t Embeddings initialised.....\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Embeddings initialised.....\n",
            "2023-11-13 18:18:40,530 | DEBUG | model.py: 89 : __init__() ::\t Building Transformer Model.....\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Building Transformer Model.....\n",
            "2023-11-13 18:18:40,980 | DEBUG | model.py: 97 : __init__() ::\t Transformer Model Built.....\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Transformer Model Built.....\n",
            "2023-11-13 18:18:40,984 | DEBUG | model.py: 103 : __init__() ::\t Initalizing Optimizer and Criterion...\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Initalizing Optimizer and Criterion...\n",
            "2023-11-13 18:18:41,357 | INFO | model.py: 109 : __init__() ::\t All Model Components Initialized...\n",
            "INFO:BERT-h10-d300-ep10_fold4:All Model Components Initialized...\n",
            "2023-11-13 18:18:41,643 | INFO | main.py: 199 : main() ::\t Initialized Model\n",
            "INFO:BERT-h10-d300-ep10_fold4:Initialized Model\n",
            "2023-11-13 18:18:41,662 | DEBUG | main.py: 216 : main() ::\t Config File Saved\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Config File Saved\n",
            "2023-11-13 18:18:41,667 | INFO | main.py: 218 : main() ::\t Starting Training Procedure\n",
            "INFO:BERT-h10-d300-ep10_fold4:Starting Training Procedure\n",
            "2023-11-13 18:18:41,672 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 1\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 1\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerModel(\n",
            "  (embedding1): BertEncoder(\n",
            "    (bert_layer): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0-11): 12 x BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pos_embedding1): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (embedding2): Embedding(30, 768)\n",
            "  (pos_embedding2): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (transformer): Transformer(\n",
            "    (encoder): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=768, out_features=1200, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=1200, out_features=768, bias=True)\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): TransformerDecoder(\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=768, out_features=1200, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=1200, out_features=768, bias=True)\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (fc_out): Linear(in_features=768, out_features=30, bias=True)\n",
            "  (criterion): CrossEntropyLoss()\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:20:52,708 | DEBUG | model.py: 460 : train_model() ::\t Training for epoch 1 completed...\n",
            "Time Taken: 2.1838818113009135\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Training for epoch 1 completed...\n",
            "Time Taken: 2.1838818113009135\n",
            "2023-11-13 18:20:52,713 | DEBUG | model.py: 461 : train_model() ::\t Starting Validation\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Starting Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:20:54,832 | DEBUG | model.py: 500 : train_model() ::\t Validation Bleu: 0.23655222396846\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Validation Bleu: 0.23655222396846\n",
            "2023-11-13 18:20:54,836 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 1\t\n",
            " best epoch: 1\t\n",
            " train loss epoch: 1.0097617037368543\t\n",
            " min train loss: 1.0097617037368543\t\n",
            " val loss epoch: 1.0867937803268433\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.2152250885179565\t\n",
            " max train acc: 0.2152250885179565\t\n",
            " val acc epoch: 0.14130434782608695\t\n",
            " max val acc: 0.14130434782608695\t\n",
            " val bleu epoch: (0.23655222396846, [0.7072784810126582, 0.30357142857142855, 0.16666666666666666, 0.0875], 1.0, 1.0360655737704918, 632, 610)\t\n",
            " max val bleu: 0.23655222396846\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 1\t\n",
            " best epoch: 1\t\n",
            " train loss epoch: 1.0097617037368543\t\n",
            " min train loss: 1.0097617037368543\t\n",
            " val loss epoch: 1.0867937803268433\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.2152250885179565\t\n",
            " max train acc: 0.2152250885179565\t\n",
            " val acc epoch: 0.14130434782608695\t\n",
            " max val acc: 0.14130434782608695\t\n",
            " val bleu epoch: (0.23655222396846, [0.7072784810126582, 0.30357142857142855, 0.16666666666666666, 0.0875], 1.0, 1.0360655737704918, 632, 610)\t\n",
            " max val bleu: 0.23655222396846\t\n",
            "2023-11-13 18:20:54,841 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 2\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 2\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:23:05,000 | DEBUG | model.py: 460 : train_model() ::\t Training for epoch 2 completed...\n",
            "Time Taken: 2.169269931316376\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Training for epoch 2 completed...\n",
            "Time Taken: 2.169269931316376\n",
            "2023-11-13 18:23:05,004 | DEBUG | model.py: 461 : train_model() ::\t Starting Validation\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Starting Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:23:07,223 | DEBUG | model.py: 500 : train_model() ::\t Validation Bleu: 0.3165115581917037\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Validation Bleu: 0.3165115581917037\n",
            "2023-11-13 18:23:07,230 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 2\t\n",
            " best epoch: 2\t\n",
            " train loss epoch: 0.6002572010261844\t\n",
            " min train loss: 0.6002572010261844\t\n",
            " val loss epoch: 1.1008691787719727\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.5035407182599899\t\n",
            " max train acc: 0.5035407182599899\t\n",
            " val acc epoch: 0.2391304347826087\t\n",
            " max val acc: 0.2391304347826087\t\n",
            " val bleu epoch: (0.3165115581917037, [0.8212996389891697, 0.3783783783783784, 0.27419354838709675, 0.17647058823529413], 0.9038579801904186, 0.9081967213114754, 554, 610)\t\n",
            " max val bleu: 0.3165115581917037\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 2\t\n",
            " best epoch: 2\t\n",
            " train loss epoch: 0.6002572010261844\t\n",
            " min train loss: 0.6002572010261844\t\n",
            " val loss epoch: 1.1008691787719727\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.5035407182599899\t\n",
            " max train acc: 0.5035407182599899\t\n",
            " val acc epoch: 0.2391304347826087\t\n",
            " max val acc: 0.2391304347826087\t\n",
            " val bleu epoch: (0.3165115581917037, [0.8212996389891697, 0.3783783783783784, 0.27419354838709675, 0.17647058823529413], 0.9038579801904186, 0.9081967213114754, 554, 610)\t\n",
            " max val bleu: 0.3165115581917037\t\n",
            "2023-11-13 18:23:07,235 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 3\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 3\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:25:16,689 | DEBUG | model.py: 460 : train_model() ::\t Training for epoch 3 completed...\n",
            "Time Taken: 2.157505464553833\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Training for epoch 3 completed...\n",
            "Time Taken: 2.157505464553833\n",
            "2023-11-13 18:25:16,696 | DEBUG | model.py: 461 : train_model() ::\t Starting Validation\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Starting Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:25:18,965 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 3\t\n",
            " best epoch: 2\t\n",
            " train loss epoch: 0.449527791503704\t\n",
            " min train loss: 0.449527791503704\t\n",
            " val loss epoch: 1.1337733268737793\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.5905412240768841\t\n",
            " max train acc: 0.5905412240768841\t\n",
            " val acc epoch: 0.2391304347826087\t\n",
            " max val acc: 0.2391304347826087\t\n",
            " val bleu epoch: (0.3258765990010558, [0.7773913043478261, 0.33248081841432225, 0.2560386473429952, 0.21739130434782608], 0.9409459638543335, 0.9426229508196722, 575, 610)\t\n",
            " max val bleu: 0.3258765990010558\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 3\t\n",
            " best epoch: 2\t\n",
            " train loss epoch: 0.449527791503704\t\n",
            " min train loss: 0.449527791503704\t\n",
            " val loss epoch: 1.1337733268737793\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.5905412240768841\t\n",
            " max train acc: 0.5905412240768841\t\n",
            " val acc epoch: 0.2391304347826087\t\n",
            " max val acc: 0.2391304347826087\t\n",
            " val bleu epoch: (0.3258765990010558, [0.7773913043478261, 0.33248081841432225, 0.2560386473429952, 0.21739130434782608], 0.9409459638543335, 0.9426229508196722, 575, 610)\t\n",
            " max val bleu: 0.3258765990010558\t\n",
            "2023-11-13 18:25:18,972 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 4\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 4\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:27:30,007 | DEBUG | model.py: 460 : train_model() ::\t Training for epoch 4 completed...\n",
            "Time Taken: 2.183836591243744\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Training for epoch 4 completed...\n",
            "Time Taken: 2.183836591243744\n",
            "2023-11-13 18:27:30,012 | DEBUG | model.py: 461 : train_model() ::\t Starting Validation\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Starting Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:27:31,911 | DEBUG | model.py: 500 : train_model() ::\t Validation Bleu: 0.31875482370644337\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Validation Bleu: 0.31875482370644337\n",
            "2023-11-13 18:27:31,915 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 4\t\n",
            " best epoch: 4\t\n",
            " train loss epoch: 0.3634048303344635\t\n",
            " min train loss: 0.3634048303344635\t\n",
            " val loss epoch: 1.136340618133545\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6170966110268084\t\n",
            " max train acc: 0.6170966110268084\t\n",
            " val acc epoch: 0.29891304347826086\t\n",
            " max val acc: 0.29891304347826086\t\n",
            " val bleu epoch: (0.31875482370644337, [0.7996688741721855, 0.3738095238095238, 0.2669491525423729, 0.1346153846153846], 0.9901154021344383, 0.9901639344262295, 604, 610)\t\n",
            " max val bleu: 0.3258765990010558\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 4\t\n",
            " best epoch: 4\t\n",
            " train loss epoch: 0.3634048303344635\t\n",
            " min train loss: 0.3634048303344635\t\n",
            " val loss epoch: 1.136340618133545\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6170966110268084\t\n",
            " max train acc: 0.6170966110268084\t\n",
            " val acc epoch: 0.29891304347826086\t\n",
            " max val acc: 0.29891304347826086\t\n",
            " val bleu epoch: (0.31875482370644337, [0.7996688741721855, 0.3738095238095238, 0.2669491525423729, 0.1346153846153846], 0.9901154021344383, 0.9901639344262295, 604, 610)\t\n",
            " max val bleu: 0.3258765990010558\t\n",
            "2023-11-13 18:27:31,919 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 5\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 5\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:29:40,571 | DEBUG | model.py: 460 : train_model() ::\t Training for epoch 5 completed...\n",
            "Time Taken: 2.1441407839457196\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Training for epoch 5 completed...\n",
            "Time Taken: 2.1441407839457196\n",
            "2023-11-13 18:29:40,576 | DEBUG | model.py: 461 : train_model() ::\t Starting Validation\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Starting Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:29:42,485 | DEBUG | model.py: 500 : train_model() ::\t Validation Bleu: 0.39864181688387956\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Validation Bleu: 0.39864181688387956\n",
            "2023-11-13 18:29:42,491 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 5\t\n",
            " best epoch: 5\t\n",
            " train loss epoch: 0.2987331022894142\t\n",
            " min train loss: 0.2987331022894142\t\n",
            " val loss epoch: 1.1189043521881104\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6102680829539706\t\n",
            " max train acc: 0.6170966110268084\t\n",
            " val acc epoch: 0.3532608695652174\t\n",
            " max val acc: 0.3532608695652174\t\n",
            " val bleu epoch: (0.39864181688387956, [0.8258620689655173, 0.43686868686868685, 0.3443396226415094, 0.25], 0.9495907867934543, 0.9508196721311475, 580, 610)\t\n",
            " max val bleu: 0.39864181688387956\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 5\t\n",
            " best epoch: 5\t\n",
            " train loss epoch: 0.2987331022894142\t\n",
            " min train loss: 0.2987331022894142\t\n",
            " val loss epoch: 1.1189043521881104\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6102680829539706\t\n",
            " max train acc: 0.6170966110268084\t\n",
            " val acc epoch: 0.3532608695652174\t\n",
            " max val acc: 0.3532608695652174\t\n",
            " val bleu epoch: (0.39864181688387956, [0.8258620689655173, 0.43686868686868685, 0.3443396226415094, 0.25], 0.9495907867934543, 0.9508196721311475, 580, 610)\t\n",
            " max val bleu: 0.39864181688387956\t\n",
            "2023-11-13 18:29:42,495 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 6\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 6\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:31:50,474 | DEBUG | model.py: 460 : train_model() ::\t Training for epoch 6 completed...\n",
            "Time Taken: 2.1329200824101764\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Training for epoch 6 completed...\n",
            "Time Taken: 2.1329200824101764\n",
            "2023-11-13 18:31:50,479 | DEBUG | model.py: 461 : train_model() ::\t Starting Validation\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Starting Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:31:52,417 | DEBUG | model.py: 500 : train_model() ::\t Validation Bleu: 0.3991020752273743\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Validation Bleu: 0.3991020752273743\n",
            "2023-11-13 18:31:52,421 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 6\t\n",
            " best epoch: 6\t\n",
            " train loss epoch: 0.26146818830659896\t\n",
            " min train loss: 0.26146818830659896\t\n",
            " val loss epoch: 1.0994855165481567\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6236722306525038\t\n",
            " max train acc: 0.6236722306525038\t\n",
            " val acc epoch: 0.358695652173913\t\n",
            " max val acc: 0.358695652173913\t\n",
            " val bleu epoch: (0.3991020752273743, [0.8253424657534246, 0.465, 0.3611111111111111, 0.21875], 0.9564559466873189, 0.9573770491803278, 584, 610)\t\n",
            " max val bleu: 0.3991020752273743\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 6\t\n",
            " best epoch: 6\t\n",
            " train loss epoch: 0.26146818830659896\t\n",
            " min train loss: 0.26146818830659896\t\n",
            " val loss epoch: 1.0994855165481567\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6236722306525038\t\n",
            " max train acc: 0.6236722306525038\t\n",
            " val acc epoch: 0.358695652173913\t\n",
            " max val acc: 0.358695652173913\t\n",
            " val bleu epoch: (0.3991020752273743, [0.8253424657534246, 0.465, 0.3611111111111111, 0.21875], 0.9564559466873189, 0.9573770491803278, 584, 610)\t\n",
            " max val bleu: 0.3991020752273743\t\n",
            "2023-11-13 18:31:52,425 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 7\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 7\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:34:02,770 | DEBUG | model.py: 460 : train_model() ::\t Training for epoch 7 completed...\n",
            "Time Taken: 2.1723650813102724\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Training for epoch 7 completed...\n",
            "Time Taken: 2.1723650813102724\n",
            "2023-11-13 18:34:02,774 | DEBUG | model.py: 461 : train_model() ::\t Starting Validation\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Starting Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:34:04,674 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 7\t\n",
            " best epoch: 6\t\n",
            " train loss epoch: 0.22196294900425004\t\n",
            " min train loss: 0.22196294900425004\t\n",
            " val loss epoch: 1.1668610572814941\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6138088012139605\t\n",
            " max train acc: 0.6236722306525038\t\n",
            " val acc epoch: 0.34782608695652173\t\n",
            " max val acc: 0.358695652173913\t\n",
            " val bleu epoch: (0.39398375670741753, [0.8033057851239669, 0.43705463182897863, 0.34177215189873417, 0.20754716981132076], 0.9917695939779199, 0.9918032786885246, 605, 610)\t\n",
            " max val bleu: 0.3991020752273743\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 7\t\n",
            " best epoch: 6\t\n",
            " train loss epoch: 0.22196294900425004\t\n",
            " min train loss: 0.22196294900425004\t\n",
            " val loss epoch: 1.1668610572814941\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6138088012139605\t\n",
            " max train acc: 0.6236722306525038\t\n",
            " val acc epoch: 0.34782608695652173\t\n",
            " max val acc: 0.358695652173913\t\n",
            " val bleu epoch: (0.39398375670741753, [0.8033057851239669, 0.43705463182897863, 0.34177215189873417, 0.20754716981132076], 0.9917695939779199, 0.9918032786885246, 605, 610)\t\n",
            " max val bleu: 0.3991020752273743\t\n",
            "2023-11-13 18:34:04,681 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 8\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 8\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:36:12,877 | DEBUG | model.py: 460 : train_model() ::\t Training for epoch 8 completed...\n",
            "Time Taken: 2.1365422010421753\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Training for epoch 8 completed...\n",
            "Time Taken: 2.1365422010421753\n",
            "2023-11-13 18:36:12,883 | DEBUG | model.py: 461 : train_model() ::\t Starting Validation\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Starting Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:36:14,797 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 8\t\n",
            " best epoch: 6\t\n",
            " train loss epoch: 0.2016012387199685\t\n",
            " min train loss: 0.2016012387199685\t\n",
            " val loss epoch: 1.265096664428711\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6102680829539706\t\n",
            " max train acc: 0.6236722306525038\t\n",
            " val acc epoch: 0.34782608695652173\t\n",
            " max val acc: 0.358695652173913\t\n",
            " val bleu epoch: (0.38361670493104266, [0.8090277777777778, 0.4336734693877551, 0.34615384615384615, 0.22580645161290322], 0.9426805832216518, 0.9442622950819672, 576, 610)\t\n",
            " max val bleu: 0.3991020752273743\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 8\t\n",
            " best epoch: 6\t\n",
            " train loss epoch: 0.2016012387199685\t\n",
            " min train loss: 0.2016012387199685\t\n",
            " val loss epoch: 1.265096664428711\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6102680829539706\t\n",
            " max train acc: 0.6236722306525038\t\n",
            " val acc epoch: 0.34782608695652173\t\n",
            " max val acc: 0.358695652173913\t\n",
            " val bleu epoch: (0.38361670493104266, [0.8090277777777778, 0.4336734693877551, 0.34615384615384615, 0.22580645161290322], 0.9426805832216518, 0.9442622950819672, 576, 610)\t\n",
            " max val bleu: 0.3991020752273743\t\n",
            "2023-11-13 18:36:14,807 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 9\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 9\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:38:23,282 | DEBUG | model.py: 460 : train_model() ::\t Training for epoch 9 completed...\n",
            "Time Taken: 2.1411928653717043\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Training for epoch 9 completed...\n",
            "Time Taken: 2.1411928653717043\n",
            "2023-11-13 18:38:23,286 | DEBUG | model.py: 461 : train_model() ::\t Starting Validation\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Starting Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:38:25,131 | DEBUG | model.py: 500 : train_model() ::\t Validation Bleu: 0.4453181563328924\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Validation Bleu: 0.4453181563328924\n",
            "2023-11-13 18:38:25,137 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 9\t\n",
            " best epoch: 9\t\n",
            " train loss epoch: 0.1830251671834802\t\n",
            " min train loss: 0.1830251671834802\t\n",
            " val loss epoch: 1.228154182434082\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6105209914011128\t\n",
            " max train acc: 0.6236722306525038\t\n",
            " val acc epoch: 0.3804347826086957\t\n",
            " max val acc: 0.3804347826086957\t\n",
            " val bleu epoch: (0.4453181563328924, [0.8356401384083045, 0.47715736040609136, 0.4, 0.3076923076923077], 0.946141331728258, 0.9475409836065574, 578, 610)\t\n",
            " max val bleu: 0.4453181563328924\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 9\t\n",
            " best epoch: 9\t\n",
            " train loss epoch: 0.1830251671834802\t\n",
            " min train loss: 0.1830251671834802\t\n",
            " val loss epoch: 1.228154182434082\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6105209914011128\t\n",
            " max train acc: 0.6236722306525038\t\n",
            " val acc epoch: 0.3804347826086957\t\n",
            " max val acc: 0.3804347826086957\t\n",
            " val bleu epoch: (0.4453181563328924, [0.8356401384083045, 0.47715736040609136, 0.4, 0.3076923076923077], 0.946141331728258, 0.9475409836065574, 578, 610)\t\n",
            " max val bleu: 0.4453181563328924\t\n",
            "2023-11-13 18:38:25,141 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 10\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 10\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:40:35,621 | DEBUG | model.py: 460 : train_model() ::\t Training for epoch 10 completed...\n",
            "Time Taken: 2.1746053377787273\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Training for epoch 10 completed...\n",
            "Time Taken: 2.1746053377787273\n",
            "2023-11-13 18:40:35,625 | DEBUG | model.py: 461 : train_model() ::\t Starting Validation\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Starting Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:40:38,238 | DEBUG | model.py: 500 : train_model() ::\t Validation Bleu: 0.4233409756596939\n",
            "DEBUG:BERT-h10-d300-ep10_fold4:Validation Bleu: 0.4233409756596939\n",
            "2023-11-13 18:40:38,246 | INFO | logger.py: 39 : print_log() ::\t \n",
            " Epoch: 10\t\n",
            " best epoch: 10\t\n",
            " train loss epoch: 0.1649149010889232\t\n",
            " min train loss: 0.1649149010889232\t\n",
            " val loss epoch: 1.3862451314926147\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6092564491654021\t\n",
            " max train acc: 0.6236722306525038\t\n",
            " val acc epoch: 0.40217391304347827\t\n",
            " max val acc: 0.40217391304347827\t\n",
            " val bleu epoch: (0.4233409756596939, [0.8725663716814159, 0.5144356955380578, 0.4263959390862944, 0.23076923076923078], 0.9234431710765795, 0.9262295081967213, 565, 610)\t\n",
            " max val bleu: 0.4453181563328924\t\n",
            "INFO:BERT-h10-d300-ep10_fold4:\n",
            " Epoch: 10\t\n",
            " best epoch: 10\t\n",
            " train loss epoch: 0.1649149010889232\t\n",
            " min train loss: 0.1649149010889232\t\n",
            " val loss epoch: 1.3862451314926147\t\n",
            " min val loss: 1.0867937803268433\t\n",
            " train acc epoch: 0.6092564491654021\t\n",
            " max train acc: 0.6236722306525038\t\n",
            " val acc epoch: 0.40217391304347827\t\n",
            " max val acc: 0.40217391304347827\t\n",
            " val bleu epoch: (0.4233409756596939, [0.8725663716814159, 0.5144356955380578, 0.4263959390862944, 0.23076923076923078], 0.9234431710765795, 0.9262295081967213, 565, 610)\t\n",
            " max val bleu: 0.4453181563328924\t\n",
            "2023-11-13 18:40:38,252 | INFO | model.py: 539 : train_model() ::\t Training Completed for 10 epochs\n",
            "INFO:BERT-h10-d300-ep10_fold4:Training Completed for 10 epochs\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 336, in set_trace\n",
            "    sys.settrace(self.trace_dispatch)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--Return--\n",
            "None\n",
            "> \u001b[0;32m/content/drive/MyDrive/Shreya Data/Shreya NLP/Project/reference/code/transformer_seq2seq/src/utils/logger.py\u001b[0m(86)\u001b[0;36mstore_results\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     84 \u001b[0;31m                        \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     85 \u001b[0;31m        \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 86 \u001b[0;31m                \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     87 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     88 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mstore_val_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> continue\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 347, in set_continue\n",
            "    sys.settrace(None)\n",
            "\n",
            "2023-11-13 18:40:42,689 | INFO | model.py: 543 : train_model() ::\t Scores saved at drive/MyDrive/Shreya Data/Shreya NLP/Project/out/val_results_cv_svamp_augmented_fold4.json\n",
            "INFO:BERT-h10-d300-ep10_fold4:Scores saved at drive/MyDrive/Shreya Data/Shreya NLP/Project/out/val_results_cv_svamp_augmented_fold4.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--Return--\n",
            "None\n",
            "> \u001b[0;32m/content/drive/MyDrive/Shreya Data/Shreya NLP/Project/reference/code/transformer_seq2seq/src/utils/logger.py\u001b[0m(122)\u001b[0;36mstore_val_results\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    118 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    119 \u001b[0;31m                \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_result_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    120 \u001b[0;31m                        \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    121 \u001b[0;31m        \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 122 \u001b[0;31m                \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> continue\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-13 18:40:45,195 | INFO | main.py: 262 : main() ::\t Final Val score: 0.08043478260869566\n",
            "INFO:BERT-h10-d300-ep10_fold4:Final Val score: 0.08043478260869566\n",
            "2023-11-13 18:40:45,198 | INFO | main.py: 263 : main() ::\t Max Training Accuracy: 0.0\n",
            "INFO:BERT-h10-d300-ep10_fold4:Max Training Accuracy: 0.0\n",
            "2023-11-13 18:40:45,202 | INFO | main.py: 264 : main() ::\t Min Training Loss: inf\n",
            "INFO:BERT-h10-d300-ep10_fold4:Min Training Loss: inf\n",
            "2023-11-13 18:40:45,204 | INFO | main.py: 265 : main() ::\t Max Val Accuracy: 0.40217391304347827\n",
            "INFO:BERT-h10-d300-ep10_fold4:Max Val Accuracy: 0.40217391304347827\n",
            "2023-11-13 18:40:45,207 | INFO | main.py: 266 : main() ::\t Min Val Loss: inf\n",
            "INFO:BERT-h10-d300-ep10_fold4:Min Val Loss: inf\n",
            "2023-11-13 18:40:45,210 | INFO | main.py: 267 : main() ::\t Val BLEU Score: 0.0\n",
            "INFO:BERT-h10-d300-ep10_fold4:Val BLEU Score: 0.0\n",
            "2023-11-13 18:40:45,213 | INFO | main.py: 268 : main() ::\t best epoch: 0\n",
            "INFO:BERT-h10-d300-ep10_fold4:best epoch: 0\n"
          ]
        }
      ],
      "source": [
        "m.main(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUAee-_k77Hu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvGkWKBr77FV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d94Qh4Fk77C1"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import shutil\n",
        "\n",
        "def unzip_gz(source_filepath, dest_filepath):\n",
        "    with gzip.open(source_filepath, 'rb') as f_in:\n",
        "        with open(dest_filepath, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "source = '/content/drive/MyDrive/Shreya Data/Shreya NLP/Project/data/GoogleNews-vectors-negative300.bin.gz'  # Replace with your .gz file path\n",
        "destination = '/content/drive/MyDrive/Shreya Data/Shreya NLP/Project/data/GoogleNews-vectors-negative300.bin'  # Replace with the destination file path\n",
        "\n",
        "unzip_gz(source, destination)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TgN18rx77Ad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf14hBsd77t4"
      },
      "source": [
        "# ROUGH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dI8n2NQwutY",
        "outputId": "5de4d1d0-c3fa-4bb3-95dd-06069b3b15af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-33-418403639afc>:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
            "<ipython-input-33-418403639afc>:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n"
          ]
        }
      ],
      "source": [
        "# from transformers import BertTokenizerFast\n",
        "\n",
        "# # Use BertTokenizerFast instead of BertTokenizer\n",
        "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# # The rest of your code remains the same\n",
        "# model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "# # Tokenize texts\n",
        "# train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "# val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# # Function to adjust labels\n",
        "# def adjust_labels(labels, tokenized_inputs):\n",
        "#     adjusted_labels = []\n",
        "#     for i, label in enumerate(labels):\n",
        "#         word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "#         label_ids = []\n",
        "#         previous_word_idx = None\n",
        "#         for word_idx in word_ids:\n",
        "#             if word_idx is None:\n",
        "#                 label_ids.append(-100)  # Special label for special tokens\n",
        "#             elif word_idx != previous_word_idx:\n",
        "#                 label_ids.append(label[word_idx])  # Label for the first token of a word\n",
        "#             else:\n",
        "#                 label_ids.append(-100)  # Special label for subsequent subtokens within a word\n",
        "#             previous_word_idx = word_idx\n",
        "#         adjusted_labels.append(label_ids)\n",
        "#     return adjusted_labels\n",
        "\n",
        "# # Adjust labels for training and validation sets\n",
        "# train_labels_list = [label.tolist() for label in train_labels]\n",
        "# val_labels_list = [label.tolist() for label in val_labels]\n",
        "\n",
        "# train_labels_list\n",
        "\n",
        "# # Now pass these lists to the adjust_labels function\n",
        "# train_labels_adjusted = adjust_labels(train_labels_list, train_encodings)\n",
        "# val_labels_adjusted = adjust_labels(val_labels_list, val_encodings)\n",
        "\n",
        "# # Convert adjusted labels to PyTorch tensors\n",
        "# train_labels_tensor = torch.tensor(train_labels_adjusted)\n",
        "# val_labels_tensor = torch.tensor(val_labels_adjusted)\n",
        "\n",
        "# # Create DataLoader\n",
        "# train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels_tensor)\n",
        "# val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels_tensor)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Use BertTokenizerFast for efficient tokenization\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Number of unique labels in your dataset\n",
        "num_labels = 10# Set this to the number of unique labels in your dataset\n",
        "\n",
        "# Use a sequence classification model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "# Tokenize texts\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# Convert the labels to tensors\n",
        "# Assuming train_labels and val_labels are lists of labels\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels_tensor)\n",
        "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jUWkgZNywulv",
        "outputId": "d2cd21d1-f5a2-4f21-d1f7-ddc82e9cb748"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-e3d3c382b48b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Extract loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1597\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 3834 is out of bounds."
          ]
        }
      ],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Assuming you have a CrossEntropyLoss for token classification\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        # Clear any previously calculated gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Unpack this training batch from our dataloader and copy tensors to GPU\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        # Extract loss\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss over the epoch\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}/{3}, Training Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    for batch in val_loader:\n",
        "        # Unpack this validation batch from our dataloader and copy tensors to GPU\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Tell the model not to compute or store gradients - saves memory and speeds up validation\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        # Extract loss\n",
        "        loss = outputs.loss\n",
        "        total_val_loss += loss.item()\n",
        "\n",
        "    # Calculate average validation loss over the epoch\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f'Epoch {epoch + 1}/{3}, Validation Loss: {avg_val_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NLaD6GowudK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Assuming val_loader is your validation dataset loader\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "for batch in val_loader:\n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_mask = batch[1].to(device)\n",
        "    labels = batch[2].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = outputs.logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "    # Convert logits to class predictions\n",
        "    batch_predictions = np.argmax(logits, axis=2)\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.extend(batch_predictions)\n",
        "    true_labels.extend(label_ids)\n",
        "\n",
        "# Flatten the predictions and true labels lists\n",
        "flat_predictions = [p for batch in predictions for p in batch]\n",
        "flat_true_labels = [l for batch in true_labels for l in batch]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Uf14hBsd77t4"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}